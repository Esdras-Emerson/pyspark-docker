{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f289f786-dc19-4127-9bdb-725efe6e960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bad5e61-80c3-4395-b4f2-d4df7404d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Alice| 34|Data Scientist|\n",
      "|  Bob| 45| Data Engineer|\n",
      "|Cathy| 29|  Data Analyst|\n",
      "|David| 35|Data Scientist|\n",
      "+-----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criando uma sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"PySpark Interview Test\").getOrCreate()\n",
    "\n",
    "# Dados fornecidos\n",
    "data = [\n",
    "    (\"Alice\", 34, \"Data Scientist\"),\n",
    "    (\"Bob\", 45, \"Data Engineer\"),\n",
    "    (\"Cathy\", 29, \"Data Analyst\"),\n",
    "    (\"David\", 35, \"Data Scientist\")\n",
    "]\n",
    "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
    "\n",
    "# Criando o DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Mostrando o DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef55a56-93e4-42e2-83f6-e395b41b6632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|David| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleção das colunas \"Name\" e \"Age\"\n",
    "selected_df = df.select(\"Name\", \"Age\")\n",
    "\n",
    "# Filtragem onde \"Age\" é maior que 30\n",
    "filtered_df = selected_df.filter(selected_df.Age > 30)\n",
    "\n",
    "# Mostrando o DataFrame filtrado\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ef76d5-a0c3-43cb-be42-88318d8901be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|    Occupation|Average_Age|\n",
      "+--------------+-----------+\n",
      "|Data Scientist|       34.5|\n",
      "| Data Engineer|       45.0|\n",
      "|  Data Analyst|       29.0|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Agrupamento por \"Occupation\" e cálculo da média de \"Age\"\n",
    "grouped_df = df.groupBy(\"Occupation\").agg(avg(\"Age\").alias(\"Average_Age\"))\n",
    "\n",
    "# Mostrando o DataFrame agrupado\n",
    "grouped_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56db1bdf-a2f0-4d0f-83a8-cf8351e606e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|    Occupation|Average_Age|\n",
      "+--------------+-----------+\n",
      "| Data Engineer|       45.0|\n",
      "|Data Scientist|       34.5|\n",
      "|  Data Analyst|       29.0|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenando em ordem decrescente pela média de \"Age\"\n",
    "sorted_df = grouped_df.orderBy(grouped_df.Average_Age.desc())\n",
    "\n",
    "# Mostrando o DataFrame ordenado\n",
    "sorted_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99fb4a87-096a-4cdf-8ef9-bfb33936a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82484f4-8ea6-4f94-82cf-7afdf97c9aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------------+\n",
      "| Name|Age|    Occupation|Age_Category|\n",
      "+-----+---+--------------+------------+\n",
      "|Alice| 34|Data Scientist|      Adulto|\n",
      "|  Bob| 45| Data Engineer|      Senior|\n",
      "|Cathy| 29|  Data Analyst|       Jovem|\n",
      "|David| 35|Data Scientist|      Adulto|\n",
      "+-----+---+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Função Python para categorizar as idades\n",
    "def categorize_age(age):\n",
    "    if age < 30:\n",
    "        return \"Jovem\"\n",
    "    elif 30 <= age <= 40:\n",
    "        return \"Adulto\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "# Convertendo a função em UDF\n",
    "categorize_age_udf = udf(categorize_age, StringType())\n",
    "\n",
    "# Aplicando a UDF ao DataFrame\n",
    "df_with_category = df.withColumn(\"Age_Category\", categorize_age_udf(df.Age))\n",
    "\n",
    "# Mostrando o DataFrame com a nova coluna\n",
    "df_with_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e95536-f3a9-4d32-8fb7-3cc88c7027ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+--------+--------------+\n",
      "| Name|Age|    Occupation|Mean_Age|Age_Difference|\n",
      "+-----+---+--------------+--------+--------------+\n",
      "|Cathy| 29|  Data Analyst|    29.0|           0.0|\n",
      "|  Bob| 45| Data Engineer|    45.0|           0.0|\n",
      "|Alice| 34|Data Scientist|    34.5|          -0.5|\n",
      "|David| 35|Data Scientist|    34.5|           0.5|\n",
      "+-----+---+--------------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Definindo a janela por \"Occupation\"\n",
    "window_spec = Window.partitionBy(\"Occupation\")\n",
    "\n",
    "# Calculando a média de \"Age\" por \"Occupation\"\n",
    "df_with_mean_age = df.withColumn(\"Mean_Age\", avg(\"Age\").over(window_spec))\n",
    "\n",
    "# Calculando a diferença entre a \"Age\" e a média\n",
    "df_with_age_diff = df_with_mean_age.withColumn(\"Age_Difference\", col(\"Age\") - col(\"Mean_Age\"))\n",
    "\n",
    "# Mostrando o DataFrame com a nova coluna\n",
    "df_with_age_diff.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8711110-2a5f-4192-8f12-e2198229d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---+-----------+\n",
      "|    Occupation| Name|Age| Department|\n",
      "+--------------+-----+---+-----------+\n",
      "|Data Scientist|Alice| 34|    Science|\n",
      "| Data Engineer|  Bob| 45|Engineering|\n",
      "|  Data Analyst|Cathy| 29|  Analytics|\n",
      "|Data Scientist|David| 35|    Science|\n",
      "+--------------+-----+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 42672)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Criando outro DataFrame para o join\n",
    "other_data = [\n",
    "    (\"Data Scientist\", \"Science\"),\n",
    "    (\"Data Engineer\", \"Engineering\"),\n",
    "    (\"Data Analyst\", \"Analytics\")\n",
    "]\n",
    "other_columns = [\"Occupation\", \"Department\"]\n",
    "\n",
    "other_df = spark.createDataFrame(other_data, schema=other_columns)\n",
    "\n",
    "# Realizando o Broadcast Join\n",
    "joined_df = df.join(broadcast(other_df), \"Occupation\")\n",
    "\n",
    "# Mostrando o DataFrame resultante do join\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b02d2-440f-44a5-8757-4f6f4a0d3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo um arquivo CSV\n",
    "csv_df = spark.read.csv(\"path/to/csv_file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Escrevendo o DataFrame em formato Parquet\n",
    "csv_df.write.parquet(\"path/to/output.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ea54b-ac52-4c92-9a75-97861176c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo um arquivo do HDFS\n",
    "hdfs_df = spark.read.csv(\"hdfs://path/to/hdfs_file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Salvando o resultado de volta no HDFS\n",
    "hdfs_df.write.csv(\"hdfs://path/to/output_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec984bf-df4f-4b8f-812c-2d2207b4b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o arquivo de log\n",
    "logs_df = spark.read.csv(\"path/to/log_file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Contando o número de ações por usuário\n",
    "actions_per_user = logs_df.groupBy(\"user_id\").count()\n",
    "\n",
    "# Encontrando os 10 usuários mais ativos\n",
    "top_users = actions_per_user.orderBy(\"count\", ascending=False).limit(10)\n",
    "\n",
    "# Salvando o resultado em um arquivo CSV\n",
    "top_users.write.csv(\"path/to/top_users.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa6079-ad2f-4bd7-8b7f-04a44db2a3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
